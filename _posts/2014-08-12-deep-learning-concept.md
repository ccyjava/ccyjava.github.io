---
layout: post
title: Deep Learning 基本概念及框架
description: 简单介绍下DL的直观概念
category: blog
---
>网上有很多DL的文章，本文不是为了全面客观的总结，只是为了简洁的列出一些框架，好让大家能瞬间想起来
>没有严谨的公式
>有很多错误还望海涵

##神经元
或者叫kernel，就是一个对输入值线性加权求和，然后经过一个sigmod函数或者其他激活函数求的一个值

##BP算法
神经元最重要的是每个边的权重，BP算法就是求权重的算法。具体来说优化目标就是根据神经网络在训练数据上的损失函数最小化，然后用梯度下降最优化权重参数。在这里引入残差的概念，残差就是实际激活值与期望激活值的差。有了残差是为了好求梯度，每个神s经元输出的残差乘以每个边的输入值就是这个边的偏导了，就可以根据这个值来更新权重。

##Auto-Encoding
神经网络和BP算法很久就有，之所以最近又火起来是因为有一些突破，其中一个就是Auto-Encoding，自编码算法就是求一层隐藏节点，这些节点能把输入值转化为另外的值，然后又能有一种转化把中间的值还原。之所以这样做，可以认为隐藏节点做的事特征提取的工作，从原始输入提取出特征，根据这些特征又能尽可能的还原原始输入。神经网络之前的问题是参数太多，不好训练（不好收敛）,还可能过拟合，而且残差会越来越小，前面的层可能更新太小。有了Auto-Encoding，就可以在正式训练前进行一个预处理，这样就会解决很多训练问题。

##线型编码器
就是前面的Auto-encoding的变形，AE经过隐藏节点处理后的数据会在经过一层节点来还原原始数据，AE的最后的节点还是普通神经元，即加权求和再过sigmod函数，线型编码器就是最后一层只加权求和不过sigmod，这样训练的会快一些

##Deep Network
最基本的DN，就是先对中间层用AE预处理，然后在一起微调。

##卷积
处理图像时经常使用，因为一张图片经常很大，所以网络的边就很多，参数就很多。全联通的网络参数学习非常耗时，也就是说收敛速度慢（防止过拟合？不变性？）。而且自然图像有其固有特性，也就是说，图像的一部分的统计特性与其他部分是一样的。这也意味着我们在这一部分学习的特征也能用在另一部分上。于是我们就不必学习一个全联通的网络，也就是说，我们可以在小分辨率的图片上学习网络参数，然后用卷积的方法用到大图片上。具体就是，把一张大图片分成很多重叠的小图片，每张小图片用之前学的的参数计算。这样每个神经元的输出就不是一个值，而是一张map。这张map可以直接作为输入给下一层处理。

##池化
经过卷积，每个神经元输出是一个map，元素太多，就要想办法减少下。可以把图片分块，每块求个均值或者最大最小值之类的，这样元素的数量就变成了分块的数量。这样做的好处是： 1 提升收敛速度 2 防止过拟合 3具有池化不变形（平移不会影响效果。池化的条件是：一个图像区域有用的特征极有可能在另一个区域同样适用（没看懂）


##reference:
[ufldl][]:	NG的deep learning教程

[tornadomeet][]:	非常好的一个技术博客




[ufldl]:    http://deeplearning.stanford.edu/wiki/index.php/UFLDL%E6%95%99%E7%A8%8B  "standford dl 教程"
[tornadomeet]:	http://www.cnblogs.com/tornadomeet/	"tornadomeet的博客"
